{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 13:40:45.964405: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-26 13:40:46.059698: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-26 13:40:46.636143: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-26 13:40:47.201398: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-26 13:40:47.643190: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-26 13:40:47.763234: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-26 13:40:48.642007: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-26 13:40:53.186613: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Data processing libraries\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import SVC classifier\n",
    "from sklearn.svm import SVC\n",
    "from SVC_optimizer_exe import SVC_Optimizer\n",
    "\n",
    "# Import XGB classifier\n",
    "import xgboost as xgb\n",
    "from XGB_optimizer_exe import XGB_optimizer\n",
    "\n",
    "# Import Gaussian classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Import metrics to compute accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Import optimizer\n",
    "import optuna\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import functools\n",
    "from functools import partial\n",
    "import joblib\n",
    "\n",
    "#Import preprocessor\n",
    "from Preprocessor_exe import Preprocessor\n",
    "from Preprocessor_caller import caller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf = datasets.gdd.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108: early stopping\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 430us/step\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438us/step\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dtf= caller(dtf, \"Label\").df\n",
    "\n",
    "\n",
    "y = dtf[\"Label\"]\n",
    "x = dtf.drop(\"Label\", axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers for SVC and XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SVC_Optimizer (X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    '''optimizes an SVC model by performing a GridSearch on fixed parameters.\n",
    "    It takes as input the result of a train-test split (X_train, X_test, y_train, y_test)\n",
    "    and returns the best model possible based on accuracy.\n",
    "    '''\n",
    "    \n",
    "    '''Initializes the SVC_Optimizer with training and test datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    X_train : array-like\n",
    "        Training feature data.\n",
    "    X_test : array-like\n",
    "        Test feature data.\n",
    "    y_train : array-like\n",
    "        Training target data.\n",
    "    y_test : array-like\n",
    "        Test target data.\n",
    "    '''\n",
    "    X_train = X_train\n",
    "    X_test = X_test\n",
    "    y_train = y_train\n",
    "    y_test = y_test\n",
    "    parameters = [{'C': [1, 10, 100, 1000], \n",
    "                        'kernel': ['rbf'], \n",
    "                        'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]},                 \n",
    "                        ]\n",
    "    \n",
    "\n",
    "    def optimizer(parameters, X_train, y_train):\n",
    "        '''Performs GridSearch to find the best hyperparameters for the SVC model.\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        dict\n",
    "            The best parameters found during the GridSearch.\n",
    "        '''\n",
    "        svc = SVC()\n",
    "        parameters = parameters  \n",
    "        \n",
    "        # Instantiate the GridSearchCV object with specified parameters\n",
    "        grid_search = GridSearchCV(estimator=svc,  \n",
    "                                    param_grid=parameters,\n",
    "                                    scoring='accuracy',\n",
    "                                    cv=5,\n",
    "                                    refit=True,\n",
    "                                    verbose=0,\n",
    "                                    n_jobs=-1)\n",
    "\n",
    "        # Fit the grid search to the training data\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Return the best parameters found\n",
    "        return grid_search.best_params_, grid_search.best_estimator_\n",
    "    \n",
    "  \n",
    "    # Get the best parameters from the optimizer\n",
    "    best_params, best_model  = optimizer(parameters, X_train, y_train)\n",
    "\n",
    "    # Print the best parameters for reference\n",
    "    print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "    return best_model\n",
    "           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_optimizer(X_train, X_test, y_train, y_test):\n",
    "    '''optimizes an XGBoost model using Optuna for hyperparameter tuning.\n",
    "It takes as input the result of a train-test split (X_train, X_test, y_train, y_test)\n",
    "and returns the best model possible based on accuracy.\n",
    "\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    X_train : array-like\n",
    "        Training feature data.\n",
    "    X_test : array-like\n",
    "        Test feature data.\n",
    "    y_train : array-like\n",
    "        Training target data.\n",
    "    y_test : array-like\n",
    "        Test target data.\n",
    "    '''\n",
    "    X_train = X_train\n",
    "    X_test = X_test\n",
    "    y_train = y_train\n",
    "    y_test = y_test\n",
    "\n",
    "    def objective(trial):\n",
    "        '''Defines the objective function for Optuna to optimize XGBoost hyperparameters.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        trial : optuna.trial.Trial\n",
    "            A single call of the objective function corresponds to one trial of the optimization.\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        float\n",
    "            The accuracy of the model with the current set of hyperparameters.\n",
    "        '''\n",
    "        # Define the hyperparameter search space\n",
    "        param = {\n",
    "            'objective': trial.suggest_categorical(\"obj\", ['reg:squarederror', 'reg:logistic', 'multi:softmax']),\n",
    "            'booster': trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]),     \n",
    "            'colsample_bynode': trial.suggest_float(\"colsample_bynode\", 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),        \n",
    "            'n_estimators': 150,        \n",
    "            'reg_alpha': trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True), # L1 regularization\n",
    "            'reg_lambda': trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True), # L2 regularization        \n",
    "            'subsample': trial.suggest_float(\"subsample\", 0.2, 1.0),        \n",
    "        }\n",
    "\n",
    "        # Additional parameters specific to 'gbtree' or 'dart' boosters\n",
    "        if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "            param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 5, step=2)\n",
    "            param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "            param[\"eta\"] =  trial.suggest_float(\"learning_rate\", 0.008, 0.2)\n",
    "            param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "            param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "        if param[\"booster\"] == \"dart\":\n",
    "            param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "            param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "            param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "            param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "        \n",
    "        # Create and train the XGBoost model with the suggested hyperparameters\n",
    "        xgb_model = xgb.XGBClassifier(**param)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict and calculate accuracy\n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        accuracy = round(accuracy_score(y_test, y_pred), 4)\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def optimization():\n",
    "        '''Performs the optimization process using Optuna to find the best hyperparameters.\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        optuna.trial.FrozenTrial\n",
    "            The best trial found by the optimization process, containing the best hyperparameters and their corresponding accuracy.\n",
    "        '''\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(partial(objective), n_trials=10, timeout=600)\n",
    "        return study.best_trial\n",
    "                \n",
    "    best_trial = optimization()\n",
    "    best_params = best_trial.params\n",
    "    \n",
    "    # Train the final model with the best hyperparameters\n",
    "    best_model = xgb.XGBClassifier(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Print best hyperparameters\n",
    "    print(f\"Best hyperparameters found: {best_params}\")\n",
    "    \n",
    "    # Return the best model and the best trial\n",
    "    return best_model, best_trial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 13:00:50,267] A new study created in memory with name: no-name-d6c16826-8bc4-4a6a-b11a-0d71212d9b92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'C': 1000, 'gamma': 0.2, 'kernel': 'rbf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 13:00:50,927] Trial 0 finished with value: 0.9238 and parameters: {'obj': 'multi:softmax', 'booster': 'gbtree', 'colsample_bynode': 0.6179065209820528, 'colsample_bytree': 0.8891933764019502, 'alpha': 2.549759591989523e-07, 'lambda': 0.0003151835935466623, 'subsample': 0.710531342207821, 'max_depth': 3, 'min_child_weight': 2, 'learning_rate': 0.1372631862063397, 'gamma': 2.14838443195522e-08, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.9238.\n",
      "[I 2024-09-26 13:01:19,434] Trial 1 finished with value: 0.909 and parameters: {'obj': 'multi:softmax', 'booster': 'dart', 'colsample_bynode': 0.8363227384301146, 'colsample_bytree': 0.6326759672576856, 'alpha': 6.564312726861813e-06, 'lambda': 0.02144911689260386, 'subsample': 0.2833391840860571, 'max_depth': 5, 'min_child_weight': 2, 'learning_rate': 0.03738092839194132, 'gamma': 0.4602095881905746, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.12634801076675134, 'skip_drop': 1.0865971256632036e-08}. Best is trial 0 with value: 0.9238.\n",
      "[I 2024-09-26 13:01:19,937] Trial 2 finished with value: 0.9204 and parameters: {'obj': 'multi:softmax', 'booster': 'gbtree', 'colsample_bynode': 0.8444280476263477, 'colsample_bytree': 0.7023068278192435, 'alpha': 0.0024389406233183636, 'lambda': 0.003192632698049671, 'subsample': 0.3231352581734081, 'max_depth': 3, 'min_child_weight': 6, 'learning_rate': 0.0974458587210606, 'gamma': 1.907482860097751e-06, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.9238.\n",
      "[I 2024-09-26 13:01:50,880] Trial 3 finished with value: 0.8705 and parameters: {'obj': 'multi:softmax', 'booster': 'dart', 'colsample_bynode': 0.7479833463025936, 'colsample_bytree': 0.5112304508102412, 'alpha': 0.0057926922170954175, 'lambda': 1.2826273810846486e-05, 'subsample': 0.9145934885651614, 'max_depth': 3, 'min_child_weight': 10, 'learning_rate': 0.01268153378052881, 'gamma': 0.06307933735407979, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.01576167182277268, 'skip_drop': 5.130893209767867e-07}. Best is trial 0 with value: 0.9238.\n",
      "[I 2024-09-26 13:02:22,207] Trial 4 finished with value: 0.9232 and parameters: {'obj': 'reg:squarederror', 'booster': 'dart', 'colsample_bynode': 0.9801493253474153, 'colsample_bytree': 0.7013107187668377, 'alpha': 8.599737286362092e-05, 'lambda': 6.33359760412903e-07, 'subsample': 0.9973406707945096, 'max_depth': 5, 'min_child_weight': 2, 'learning_rate': 0.05362672834403321, 'gamma': 0.8939165131635785, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.005622681138890079, 'skip_drop': 3.5783048906968513e-07}. Best is trial 0 with value: 0.9238.\n",
      "[I 2024-09-26 13:02:54,221] Trial 5 finished with value: 0.9245 and parameters: {'obj': 'multi:softmax', 'booster': 'dart', 'colsample_bynode': 0.6390826572583312, 'colsample_bytree': 0.5730892982843581, 'alpha': 3.864452823903277e-05, 'lambda': 2.2178330898732097e-08, 'subsample': 0.7224249106362277, 'max_depth': 3, 'min_child_weight': 2, 'learning_rate': 0.13369029205824148, 'gamma': 4.684015957578627e-07, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.00366048437191052, 'skip_drop': 0.6363802560163943}. Best is trial 5 with value: 0.9245.\n",
      "[I 2024-09-26 13:03:28,584] Trial 6 finished with value: 0.9217 and parameters: {'obj': 'reg:squarederror', 'booster': 'dart', 'colsample_bynode': 0.9297792519171151, 'colsample_bytree': 0.7741819251628157, 'alpha': 0.11552504442763133, 'lambda': 0.15908020156900188, 'subsample': 0.4913512751432504, 'max_depth': 5, 'min_child_weight': 9, 'learning_rate': 0.14265976830874055, 'gamma': 0.2058052204455613, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 4.001415240869935e-07, 'skip_drop': 5.5369105740930245e-05}. Best is trial 5 with value: 0.9245.\n",
      "[I 2024-09-26 13:04:01,438] Trial 7 finished with value: 0.9255 and parameters: {'obj': 'multi:softmax', 'booster': 'dart', 'colsample_bynode': 0.7198482630816196, 'colsample_bytree': 0.8268351867101851, 'alpha': 0.06787040962310842, 'lambda': 0.7727632129029408, 'subsample': 0.5901322670451963, 'max_depth': 5, 'min_child_weight': 3, 'learning_rate': 0.1819203271783996, 'gamma': 0.1410110509660484, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 1.3924930835341188e-07, 'skip_drop': 1.5926816839954922e-06}. Best is trial 7 with value: 0.9255.\n",
      "[I 2024-09-26 13:04:01,968] Trial 8 finished with value: 0.926 and parameters: {'obj': 'reg:logistic', 'booster': 'gbtree', 'colsample_bynode': 0.9499189514019735, 'colsample_bytree': 0.8860527795097257, 'alpha': 0.0003110064036822101, 'lambda': 0.6152146730576613, 'subsample': 0.55026940355463, 'max_depth': 5, 'min_child_weight': 5, 'learning_rate': 0.15874833519225046, 'gamma': 7.738208384103265e-05, 'grow_policy': 'depthwise'}. Best is trial 8 with value: 0.926.\n",
      "[I 2024-09-26 13:04:28,656] Trial 9 finished with value: 0.9178 and parameters: {'obj': 'reg:logistic', 'booster': 'dart', 'colsample_bynode': 0.8635794033323672, 'colsample_bytree': 0.7413266822626958, 'alpha': 9.292029474052184e-08, 'lambda': 0.07445248954069676, 'subsample': 0.6473777217055112, 'max_depth': 5, 'min_child_weight': 10, 'learning_rate': 0.06613474842250665, 'gamma': 1.9307742750128428e-07, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.3742457775483982, 'skip_drop': 0.04886814706650991}. Best is trial 8 with value: 0.926.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found: {'obj': 'reg:logistic', 'booster': 'gbtree', 'colsample_bynode': 0.9499189514019735, 'colsample_bytree': 0.8860527795097257, 'alpha': 0.0003110064036822101, 'lambda': 0.6152146730576613, 'subsample': 0.55026940355463, 'max_depth': 5, 'min_child_weight': 5, 'learning_rate': 0.15874833519225046, 'gamma': 7.738208384103265e-05, 'grow_policy': 'depthwise'}\n"
     ]
    }
   ],
   "source": [
    "svc = SVC_Optimizer(X_train, X_test, y_train, y_test)\n",
    "xgb = XGB_optimizer(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Executer:\n",
    "    '''This class creates and optimizes three models: XGBoost (XGB), Support Vector Classifier (SVC), and Naive Bayes (NB).\n",
    "    The models are optimized and trained on the provided dataset. The class also includes methods for saving these models.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The input dataframe containing the data.\n",
    "    df_imputed : pd.DataFrame\n",
    "        The imputed dataframe after preprocessing.\n",
    "    X_train : pd.DataFrame\n",
    "        The training data features.\n",
    "    X_test : pd.DataFrame\n",
    "        The test data features.\n",
    "    y_train : pd.Series\n",
    "        The training data labels.\n",
    "    y_test : pd.Series\n",
    "        The test data labels.\n",
    "    metrics_dict : dict\n",
    "        A dictionary containing performance metrics for each model.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    preprocess(df, limit_try=3000, target=\"Label\"):\n",
    "        Preprocesses the input dataframe by dropping the \"Timestamp\" column and imputing missing values.\n",
    "    split(df, label):\n",
    "        Splits the dataframe into training and test sets.\n",
    "    optimize_model(df, label=\"Label\"):\n",
    "        Optimizes and returns three models: XGBoost, SVC, and Naive Bayes.\n",
    "    metrics(y_pred):\n",
    "        Calculates and returns performance metrics (accuracy, RMSE, MAE, R2) for the predicted values.\n",
    "    model_creator():\n",
    "        Creates and trains the models, then returns a dictionary of performance metrics.\n",
    "    saver(model_to_save, path, filename):\n",
    "        Saves the specified model as a pickle file in the given path with a timestamped filename.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, df, label: str = \"Label\") -> None:\n",
    "        '''Initializes the Model_Executer class with the provided dataframe and target label.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            The input dataframe.\n",
    "        label : str, optional\n",
    "            The name of the target column to predict, default is \"Label\".\n",
    "        '''\n",
    "        self.df = df\n",
    "        self.df_imputed = self.preprocess(self.df)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.split(self.df_imputed, label)\n",
    "        self.metrics_dict = self.model_creator()\n",
    "\n",
    "    def preprocess(self, df, limit_try: int = 3000, target: str = \"Label\"):\n",
    "        '''Preprocesses the input dataframe by dropping the \"Timestamp\" column and imputing missing values.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            The input dataframe to preprocess.\n",
    "        limit_try : int, optional\n",
    "            The number of rows to use for initial preprocessing, default is 3000.\n",
    "        target : str, optional\n",
    "            The target column name, default is \"Label\".\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            The imputed dataframe.\n",
    "        '''\n",
    "        if \"Timestamp\" in df.columns:\n",
    "            df.drop(\"Timestamp\", axis=1, inplace=True)\n",
    "        \n",
    "        df_veri = df.iloc[:limit_try, :]\n",
    "        df_try = df.iloc[limit_try:, :]\n",
    "            \n",
    "        obj = Preprocessor(df=df_veri, target=target)\n",
    "        df_imputed = obj.predictor(df_try)\n",
    "\n",
    "        return df_imputed\n",
    "            \n",
    "    def split(self, df, label):\n",
    "        '''Splits the dataframe into training and test sets.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            The input dataframe to split.\n",
    "        label : str\n",
    "            The name of the target column.\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        tuple\n",
    "            A tuple containing the training and test features and labels (X_train, X_test, y_train, y_test).\n",
    "        '''\n",
    "        X = df.drop(label, axis=1).copy()\n",
    "        y = df[label]\n",
    "\n",
    "        # Split the data into training and test sets\n",
    "        X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def optimize_model(self, df: pd.DataFrame, label: str = \"Label\"):\n",
    "        '''Optimizes and returns three models: XGBoost, SVC, and Naive Bayes.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            The imputed dataframe.\n",
    "        label : str, optional\n",
    "            The target column name, default is \"Label\".\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        tuple\n",
    "            A tuple containing the optimized models (XGBoost, SVC, Naive Bayes).\n",
    "        '''\n",
    "\n",
    "        def xgboost():\n",
    "            print(\"Entering optimization 1\")\n",
    "            opti = XGB_optimizer(self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "            trial = opti.optimization()\n",
    "            xgb_model = xgb.XGBClassifier(**trial.params)\n",
    "            print(f\"XGB model created with params: {xgb_model.get_params()}\")\n",
    "            return xgb_model\n",
    "        \n",
    "        def svc():\n",
    "            print(\"Entering optimization 2\")\n",
    "            svc_o = SVC_Optimizer(self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "            best_svc_model = svc_o.svc_opt_model()\n",
    "            print(\"SVC model created\")\n",
    "            return best_svc_model\n",
    "        \n",
    "        def naive_bayes():\n",
    "            print(\"Entering optimization 3\")\n",
    "            gnb = GaussianNB()\n",
    "            print(\"Naive Bayes model created\") \n",
    "            return gnb\n",
    "        \n",
    "        xgb_o, svc_o, naive_bayes_o = xgboost(), svc(), naive_bayes()\n",
    "\n",
    "        return xgb_o, svc_o, naive_bayes_o\n",
    "    \n",
    "    def metrics(self, y_pred):\n",
    "        '''Calculates and returns performance metrics (accuracy, RMSE, MAE, R2) for the predicted values.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        y_pred : array-like\n",
    "            The predicted values.\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary containing the calculated metrics.\n",
    "        '''\n",
    "        accuracy = round(accuracy_score(self.y_test, y_pred), 4)\n",
    "        rmse = mean_squared_error(self.y_test, y_pred, squared=False)\n",
    "        mae = mean_absolute_error(self.y_test, y_pred)\n",
    "        r2 = r2_score(self.y_test, y_pred)\n",
    "\n",
    "        metrics_dict = {\"accuracy\": accuracy, \"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
    "\n",
    "        return metrics_dict\n",
    "\n",
    "    def model_creator(self):\n",
    "        '''Creates and trains the models, then returns a dictionary of performance metrics.\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary containing performance metrics for each model.\n",
    "        '''\n",
    "        xgb_o, svc_o, naive_bayes_o = self.optimize_model(self.df_imputed, \"Label\")        \n",
    "\n",
    "        metrics_dict = {}\n",
    "\n",
    "        # Train and evaluate the XGBoost model\n",
    "        xgb_model = xgb_o            \n",
    "        booster = xgb_model.fit(self.X_train, self.y_train) \n",
    "        self.booster = booster\n",
    "        y_pred_xgb = booster.predict(self.X_test)\n",
    "        metrics_dict[\"XGB\"] = self.metrics(y_pred_xgb)\n",
    "\n",
    "        # Train and evaluate the SVC model\n",
    "        svc = svc_o\n",
    "        svc_fit = svc.fit(self.X_train, self.y_train)\n",
    "        self.svc = svc_fit\n",
    "        y_pred_svc = svc_fit.predict(self.X_test)\n",
    "        metrics_dict[\"SVC\"] = self.metrics(y_pred_svc)                \n",
    "\n",
    "        # Train and evaluate the Naive Bayes model\n",
    "        nb = naive_bayes_o\n",
    "        nb_fit = nb.fit(self.X_train, self.y_train)\n",
    "        self.nb = nb_fit\n",
    "        y_pred_nb = nb_fit.predict(self.X_test)\n",
    "        metrics_dict[\"NB\"] = self.metrics(y_pred_nb)  \n",
    "\n",
    "        return metrics_dict\n",
    "    \n",
    "    def saver(self, model_to_save, path, filename):\n",
    "        '''Saves the specified model as a pickle file in the given path with a timestamped filename.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        model_to_save : object\n",
    "            The model to be saved.\n",
    "        path : str\n",
    "            The directory path where the model will be saved.\n",
    "        filename : str\n",
    "            The name of the file to save the model as.\n",
    "        '''\n",
    "        cwd = os.getcwd()\n",
    "        path = os.path.join(cwd, path)\n",
    "        date = datetime.datetime.now().strftime(\"%d_%m_%Y\")\n",
    "\n",
    "        if os.path.exists(f\"{path}\\\\{date}\"):            \n",
    "            try:                 \n",
    "                joblib.dump(model_to_save, f\"{path}\\\\{date}\\\\{filename}.pkl\")                \n",
    "            except:                \n",
    "                pass\n",
    "        else:          \n",
    "            os.mkdir(f\"{path}\\\\{date}\") \n",
    "            print(f\"Directory {path} created at {cwd}\") \n",
    "            joblib.dump(model_to_save, f\"{path}\\\\{date}\\\\{filename}.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727350860.461053  791698 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-26 13:41:00.470112: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161: early stopping\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step\n",
      "\u001b[1m414/414\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 13:41:05,469] A new study created in memory with name: no-name-7097583e-e3c7-4963-9154-7dbea1d6c468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering optimization 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 13:41:11,518] Trial 0 finished with value: 0.7813 and parameters: {'obj': 'reg:squarederror', 'booster': 'dart', 'colsample_bynode': 0.5438463507638324, 'colsample_bytree': 0.9948004339427853, 'alpha': 0.047092244786536716, 'lambda': 0.046866331074994196, 'subsample': 0.2641420452002266, 'max_depth': 3, 'min_child_weight': 3, 'learning_rate': 0.02049990266905863, 'gamma': 0.33566335509193096, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.7633198542737005, 'skip_drop': 0.0032921318906337167}. Best is trial 0 with value: 0.7813.\n",
      "[I 2024-09-26 13:41:35,430] Trial 1 finished with value: 0.8782 and parameters: {'obj': 'multi:softmax', 'booster': 'dart', 'colsample_bynode': 0.779974468394286, 'colsample_bytree': 0.740388351407157, 'alpha': 7.545454408390574e-08, 'lambda': 0.01966373258814042, 'subsample': 0.6706771270190572, 'max_depth': 3, 'min_child_weight': 4, 'learning_rate': 0.06646748902823546, 'gamma': 0.014361465147437589, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.0021892800278746046, 'skip_drop': 0.016651582908029525}. Best is trial 1 with value: 0.8782.\n",
      "[I 2024-09-26 13:41:35,684] Trial 2 finished with value: 0.8836 and parameters: {'obj': 'reg:squarederror', 'booster': 'gbtree', 'colsample_bynode': 0.9363834644824394, 'colsample_bytree': 0.6371818059985648, 'alpha': 0.006157482625407876, 'lambda': 5.4227295716069495e-06, 'subsample': 0.9225993525974119, 'max_depth': 5, 'min_child_weight': 7, 'learning_rate': 0.1938813049695605, 'gamma': 0.7122003825866694, 'grow_policy': 'depthwise'}. Best is trial 2 with value: 0.8836.\n",
      "[I 2024-09-26 13:41:36,379] Trial 3 finished with value: 0.8821 and parameters: {'obj': 'reg:logistic', 'booster': 'gbtree', 'colsample_bynode': 0.7709777244903999, 'colsample_bytree': 0.9938194531448435, 'alpha': 0.008892044188361016, 'lambda': 0.0037523219413114544, 'subsample': 0.735466422570688, 'max_depth': 5, 'min_child_weight': 8, 'learning_rate': 0.055367673396047944, 'gamma': 0.06921352432078673, 'grow_policy': 'lossguide'}. Best is trial 2 with value: 0.8836.\n",
      "[I 2024-09-26 13:41:36,751] Trial 4 finished with value: 0.885 and parameters: {'obj': 'multi:softmax', 'booster': 'gbtree', 'colsample_bynode': 0.9145906516465411, 'colsample_bytree': 0.5440205518293681, 'alpha': 1.9854424030076934e-06, 'lambda': 1.1672989354917992e-05, 'subsample': 0.4758746065540871, 'max_depth': 3, 'min_child_weight': 9, 'learning_rate': 0.15842430804725546, 'gamma': 0.002863656426177596, 'grow_policy': 'depthwise'}. Best is trial 4 with value: 0.885.\n",
      "[I 2024-09-26 13:41:37,257] Trial 5 finished with value: 0.8831 and parameters: {'obj': 'multi:softmax', 'booster': 'gbtree', 'colsample_bynode': 0.5310442131954985, 'colsample_bytree': 0.5596113622572154, 'alpha': 8.909441417701704e-05, 'lambda': 0.22011222941391537, 'subsample': 0.8545091317890954, 'max_depth': 5, 'min_child_weight': 8, 'learning_rate': 0.0712030019204892, 'gamma': 0.0017329617849311796, 'grow_policy': 'lossguide'}. Best is trial 4 with value: 0.885.\n",
      "[I 2024-09-26 13:42:01,230] Trial 6 finished with value: 0.8816 and parameters: {'obj': 'reg:logistic', 'booster': 'dart', 'colsample_bynode': 0.6496445764352351, 'colsample_bytree': 0.9662128053837372, 'alpha': 0.021339375787175173, 'lambda': 0.05348487440931021, 'subsample': 0.48875491311993696, 'max_depth': 3, 'min_child_weight': 7, 'learning_rate': 0.15757536311669013, 'gamma': 6.762230754857276e-06, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.00038952331808037196, 'skip_drop': 1.8758872056538508e-08}. Best is trial 4 with value: 0.885.\n",
      "[I 2024-09-26 13:42:01,565] Trial 7 finished with value: 0.8796 and parameters: {'obj': 'multi:softmax', 'booster': 'gbtree', 'colsample_bynode': 0.5101355158219438, 'colsample_bytree': 0.9181915257726685, 'alpha': 1.2023591384841222e-05, 'lambda': 3.73066041539424e-07, 'subsample': 0.5458883039818874, 'max_depth': 3, 'min_child_weight': 6, 'learning_rate': 0.09279915277609305, 'gamma': 0.07608058824902031, 'grow_policy': 'lossguide'}. Best is trial 4 with value: 0.885.\n",
      "[I 2024-09-26 13:42:01,974] Trial 8 finished with value: 0.886 and parameters: {'obj': 'reg:squarederror', 'booster': 'gbtree', 'colsample_bynode': 0.7987432838017517, 'colsample_bytree': 0.7997256981170161, 'alpha': 0.0003221228496444942, 'lambda': 0.16260395973052968, 'subsample': 0.5039035443834982, 'max_depth': 5, 'min_child_weight': 10, 'learning_rate': 0.127212426454724, 'gamma': 2.848076242548765e-08, 'grow_policy': 'depthwise'}. Best is trial 8 with value: 0.886.\n",
      "[I 2024-09-26 13:42:02,299] Trial 9 finished with value: 0.8576 and parameters: {'obj': 'reg:squarederror', 'booster': 'gbtree', 'colsample_bynode': 0.6034604200363958, 'colsample_bytree': 0.8046751929421105, 'alpha': 0.000358309400639682, 'lambda': 0.6949839879704098, 'subsample': 0.2638828036316572, 'max_depth': 3, 'min_child_weight': 6, 'learning_rate': 0.021718078948372396, 'gamma': 0.002749390633307191, 'grow_policy': 'lossguide'}. Best is trial 8 with value: 0.886.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB model created with params: {'objective': 'binary:logistic', 'base_score': None, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': 0.7987432838017517, 'colsample_bytree': 0.7997256981170161, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': 2.848076242548765e-08, 'grow_policy': 'depthwise', 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.127212426454724, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 5, 'max_leaves': None, 'min_child_weight': 10, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.5039035443834982, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'obj': 'reg:squarederror', 'alpha': 0.0003221228496444942, 'lambda': 0.16260395973052968}\n",
      "Entering optimization 2\n",
      "{'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "SVC model created\n",
      "Entering optimization 3\n",
      "Naive Bayes model created\n"
     ]
    }
   ],
   "source": [
    "dtf = datasets.gdd.load_data()\n",
    "model_ex = Model_Executer(dtf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving the optimized models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'XGB': {'accuracy': 0.8792,\n",
       "  'rmse': 1.7009839445590746,\n",
       "  'mae': 0.4334637964774951,\n",
       "  'r2': 0.28149639610429167},\n",
       " 'SVC': {'accuracy': 0.8635,\n",
       "  'rmse': 1.7606261117238258,\n",
       "  'mae': 0.46966731898238745,\n",
       "  'r2': 0.2302267781056463},\n",
       " 'NB': {'accuracy': 0.5044,\n",
       "  'rmse': 2.4568686416869934,\n",
       "  'mae': 1.3091976516634052,\n",
       "  'r2': -0.49896812053859474}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Seeing the metrics \n",
    "model_ex.metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = model_ex.booster\n",
    "svc = model_ex.svc\n",
    "nb = model_ex.nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xgb.predict(X_test)\n",
    "\n",
    "\n",
    "true_percentage = [\"True:\", sum(1 for i, p in zip(pred, y_test) if p == i)/len(y_test)*100, \"False:\", sum(1 for i, p in zip(pred, y_test) if p != i)/len(y_test)*100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['True:', 81.33756771903606, 'False:', 18.662432280963944]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
